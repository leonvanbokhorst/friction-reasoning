# Model Configuration
model_config:
  base_model: "unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit"
  trust_remote_code: false
  use_flash_attention_2: true
  torch_dtype: "bfloat16"
  device_map: "auto"
  model_max_length: 4096

# LoRA Configuration
lora_config:
  r: 32 
  lora_alpha: 64
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"
  lora_dropout: 0.0
  inference_mode: false

# Dataset Configuration
dataset_config:
  datasets:
    - name: "leonvanbokhorst/friction-disagreement-v2"
      weight: 0.25
    - name: "leonvanbokhorst/friction-overthinking-v2"
      weight: 0.10
    - name: "leonvanbokhorst/reluctance-v6.1"
      weight: 0.25
    - name: "leonvanbokhorst/reluctance-v5.3"
      weight: 0.15
    - name: "leonvanbokhorst/friction-uncertainty-v2"
      weight: 0.25
  shuffle_seed: 42
  columns:
    question: "question"
    agent_responses: "agent_responses"
    final_answer: "final_answer"
  format_template: | 
    <|im_start|>system
    You are a human-like AI assistant.
    <|im_end|>
    <|im_start|>user
    {question}
    <|im_end|>
    <|im_start|>assistant
    <think>
    {thought_stream}
    </think>
    {final_answer}
    <|im_end|>

# Training Configuration
training_config:
  learning_rate: 2e-4
  num_train_epochs: 7
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  warmup_ratio: 0.1
  logging_steps: 10
  evaluation_strategy: "steps"
  eval_steps: 50
  save_strategy: "steps"
  save_steps: 50
  save_total_limit: 5
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  early_stopping_patience: 3
  early_stopping_threshold: 0.005
  gradient_checkpointing: true
  optim: "adamw_8bit"
  fp16: false
  bf16: true
  report_to: ["wandb"]
  weight_decay: 0.01
  max_grad_norm: 0.5

# Output Configuration
output_config:
  output_dir: "models/friction_reasoning"
  logging_dir: "logs/friction_reasoning" 